import os
import streamlit as st
import nest_asyncio
import tempfile
from pathlib import Path

# Streamlitì—ì„œ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
nest_asyncio.apply()

from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_core.output_parsers import StrOutputParser

# RAG ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# --- 1. Gemini API í‚¤ ì„¤ì • ---
try:
    # âš ï¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    PDF_FILE_PATH = "ëª…ì‹ ì—¬ê³ ì†Œê°œ.pdf" 
    
    # âš ï¸ API í‚¤ ì„¤ì • í™•ì¸
    if "GOOGLE_API_KEY" not in st.secrets:
        raise ValueError("GOOGLE_API_KEYê°€ Streamlit Secretsì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception as e:
    st.error(f"âš ï¸ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
    st.info("ğŸ’¡ `st.secrets` íŒŒì¼ì— `GOOGLE_API_KEY`ë¥¼ ì„¤ì •í•˜ê³ , `ëª…ì‹ ì—¬ê³ ì†Œê°œ.pdf` íŒŒì¼ì´ ì½”ë“œì™€ ê°™ì€ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()


# --- 2. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• (ìºì‹œ) ---
@st.cache_resource(show_spinner="ğŸ“š í•™êµ ì†Œê°œ ë¬¸ì„œ ë¡œë”© ë° í•™ìŠµ ì¤‘...")
def get_retriever(pdf_path: str):
    """
    PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , ë¶„í• í•˜ê³ , ì„ë² ë”©í•˜ì—¬ FAISS ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰ê¸°(Retriever)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not Path(pdf_path).exists():
        st.error(f"âŒ '{pdf_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    try:
        # 1. ë¬¸ì„œ ë¡œë“œ (PyPDFLoader ì‚¬ìš©)
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        # 2. í…ìŠ¤íŠ¸ ë¶„í•  (RecursiveCharacterTextSplitter ì‚¬ìš©)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
            add_start_index=True
        )
        texts = text_splitter.split_documents(documents)

        # 3. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (Gemini ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©)
        # embedding_model = GoogleGenerativeAIEmbeddings(model="embedding-001") # ì„ë² ë”© ë¹„ìš©ì„ ì¤„ì´ê¸° ìœ„í•´ ê¶Œì¥
        embedding_model = GoogleGenerativeAIEmbeddings(model="text-embedding-004")

        # 4. ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ì €ì¥
        # FAISS: ë¡œì»¬ì—ì„œ ë¹ ë¥´ê³  ê°„ë‹¨í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë²¡í„° ì €ì¥ì†Œ
        vectorstore = FAISS.from_documents(texts, embedding_model)

        # 5. ê²€ìƒ‰ê¸°(Retriever) ì„¤ì •
        # k=3: ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ë†’ì€ 3ê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        
        return retriever

    except Exception as e:
        st.error(f"âŒ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")
        st.stop()

# --- 3. LLM ë° RAG ì²´ì¸ ì„¤ì • (ìºì‹œ) ---
@st.cache_resource(show_spinner="ğŸ¤– ì±—ë´‡ ëª¨ë¸ ë¡œë”© ì¤‘...")
def get_rag_chain(selected_model, retriever):
    """
    LLM, í”„ë¡¬í”„íŠ¸, ê²€ìƒ‰ê¸°ë¥¼ ê²°í•©í•œ RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        llm = ChatGoogleGenerativeAI(
            model=selected_model,
            temperature=0.3, # ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„ ì„¤ì •
            convert_system_message_to_human=True
        )
    except Exception as e:
        st.error(f"âŒ Gemini ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        st.stop()

    # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì • (RAGìš©)
    SYSTEM_PROMPT = (
        "ë‹¹ì‹ ì€ ëª…ì‹ ì—¬ê³  ì†Œê°œ ì „ë¬¸ê°€ 'ëª…ì‹ AI'ì…ë‹ˆë‹¤. "
        "í•­ìƒ í•œêµ­ì–´ì™€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. "
        "ì œê³µëœ **ë¬¸ë§¥(context)** ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. "
        "ë¬¸ë§¥ì— ë‹µë³€í•  ì •ë³´ê°€ ì—†ë‹¤ë©´, 'ì£„ì†¡í•˜ì§€ë§Œ, ì œê°€ ê°€ì§„ ëª…ì‹ ì—¬ê³  ì†Œê°œ ìë£Œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”. "
        "ëŒ€í™”ì— ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì„ì–´ ë‹µí•´ì£¼ì„¸ìš”. ğŸ“\n\n"
        "**ë¬¸ë§¥(Context):**\n{context}\n\n"
    )
    
    # 2. ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
    question_answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", SYSTEM_PROMPT),
            MessagesPlaceholder("history"), # ëŒ€í™” ê¸°ë¡ (RAGì—ì„œë„ í•„ìˆ˜)
            ("human", "{input}"),        # ì‚¬ìš©ìì˜ í˜„ì¬ ì…ë ¥
        ]
    )

    # 3. ë¬¸ì„œ ê²°í•© ì²´ì¸ (ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ì˜ context ë³€ìˆ˜ì— ë„£ìŒ)
    document_chain = create_stuff_documents_chain(llm, question_answer_prompt)
    
    # 4. ê²€ìƒ‰ ì²´ì¸ (ê²€ìƒ‰ê¸° + ë¬¸ì„œ ê²°í•© ì²´ì¸)
    # create_retrieval_chainì€ {input}ìœ¼ë¡œ ì§ˆë¬¸ì„ ë°›ê³ , ê²€ìƒ‰ê¸°ë¡œ ë¬¸ì„œë¥¼ ì°¾ì€ ë’¤, document_chainì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    rag_chain = create_retrieval_chain(retriever, document_chain)
    
    return rag_chain


# --- 4. Streamlit UI ì„¤ì • ---

st.header("ëª…ì‹ ì—¬ìê³ ë“±í•™êµ ì†Œê°œ AI ì±—ë´‡ ğŸ“")
st.info("ëª…ì‹ ì—¬ê³ ì†Œê°œ.pdf ê¸°ë°˜ì˜ ì „ë¬¸ AIì…ë‹ˆë‹¤. í•™êµì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.")

# ì±„íŒ… ê¸°ë¡ì„ Streamlitì˜ ì„¸ì…˜ ìƒíƒœ(session_state)ì— ì €ì¥
chat_history = StreamlitChatMessageHistory(key="chat_messages")

# ëª¨ë¸ ì„ íƒ
option = st.selectbox("Select Gemini Model",
    ("gemini-2.5-flash", "gemini-2.5-pro"),
    index=0,
    help="ê°€ì¥ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ 2.5 Flash ëª¨ë¸ì„ ì¶”ì²œí•©ë‹ˆë‹¤."
)

# RAG ê²€ìƒ‰ê¸° ê°€ì ¸ì˜¤ê¸°
rag_retriever = get_retriever(PDF_FILE_PATH)

# RAG ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
rag_chain = get_rag_chain(option, rag_retriever)

# ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•˜ëŠ” Runnable ìƒì„±
# LangChain RAG ì²´ì¸ê³¼ ëŒ€í™” ê¸°ë¡ì„ ê²°í•©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
)

# --- 5. ì±„íŒ… UI ë¡œì§ ---

# ì²« ë°©ë¬¸ ì‹œ í™˜ì˜ ë©”ì‹œì§€ ì¶”ê°€
if not chat_history.messages:
    chat_history.add_ai_message("ëª…ì‹ ì—¬ìê³ ë“±í•™êµ ì†Œê°œ ì „ë¬¸ AI, ëª…ì‹ AIì…ë‹ˆë‹¤! ğŸ˜Š ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.")

# ì´ì „ ëŒ€í™” ê¸°ë¡ ëª¨ë‘ ì¶œë ¥
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
if prompt_message := st.chat_input("ëª…ì‹ ì—¬ê³ ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€ ì¶œë ¥
    st.chat_message("human").write(prompt_message)
    
    # AI ì‘ë‹µ ìƒì„± ë° ì¶œë ¥
    with st.chat_message("ai"):
        with st.spinner("ëª…ì‹ ì—¬ê³  ìë£Œì—ì„œ ë‹µë³€ì„ ì°¾ëŠ” ì¤‘..."):
            config = {"configurable": {"session_id": "any_id"}}
            
            # ì²´ì¸ ì‹¤í–‰
            # RAG ì²´ì¸ì˜ ê²°ê³¼ëŠ” ë”•ì…”ë„ˆë¦¬({answer, context, input}) í˜•íƒœë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.
            # ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ê²ƒì€ 'answer
